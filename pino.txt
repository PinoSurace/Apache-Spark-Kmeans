
import org.apache.spark.SparkConf

import org.apache.spark.sql.SQLContext
import org.apache.spark.sql.Column
import org.apache.spark.sql.Row
import org.apache.spark.sql.DataFrame
import org.apache.spark.sql.types.StructType
import org.apache.spark.sql.types.{ArrayType, StringType, StructField}
import org.apache.spark.sql.types.DataType
import org.apache.spark.sql.functions.col
import org.apache.spark.sql.functions.{sum, min, max, asc, desc, udf, mean}
import org.apache.spark.sql.functions.coalesce
import org.apache.spark.sql.functions.explode
import org.apache.spark.sql.functions.array
import org.apache.spark.sql.SparkSession

import com.databricks.spark.xml._
import org.apache.spark.storage.StorageLevel
import org.apache.spark.streaming.{Seconds, StreamingContext}

import java.lang.Thread

import org.apache.log4j.Logger
import org.apache.log4j.Level
import org.apache.spark.ml.clustering.KMeans
import org.apache.spark.ml.feature.VectorAssembler
import org.apache.spark.ml.Pipeline
import org.apache.spark.ml.feature.StringIndexer
import org.apache.spark.sql.SQLContext
import org.apache.spark.SparkConf
import org.apache.spark.SparkContext
import org.apache.spark.SparkContext._
import org.apache.spark.rdd.RDD
import org.apache.spark.sql.types.{StructField,StructType, StringType, LongType, IntegerType, DoubleType, Metadata}
import org.apache.spark.sql.SparkSession

//import org.apache.spark.sql.SQLContext.implicits._
//import org.apache.spark.ml.feature.Imputer

object main extends App {
  
  //menu
/*  if (args.length == 0) {
        println("error, parameters missing")
   }
  else if (args(0) == "task"){
    args(1) match {
      case 1 =>
      case 2 =>
      case 3 =>
      case 4 =>
      case 5 =>
      case 6 =>  
      
    }
    
  } */
  

  // Suppress the log messages:
  Logger.getLogger("org").setLevel(Level.OFF)

	val spark = SparkSession.builder()
                          .appName("ex2")
                          .config("spark.driver.host", "localhost")
                          .master("local")
                          .getOrCreate()

  spark.conf.set("spark.sql.shuffle.partitions", "5")
  
  import spark.implicits._
  //task 1
  val df1 : DataFrame  = spark.read 
                   .format("csv")
                   .option("delimiter", ";")
                   .option("header", "true")
                   .option("inferSchema", "true")
                   .load("data/tieliikenneonnettomuudet_2015_hlo.csv")
                   
 //df1.printSchema()
 val df2 : DataFrame  = spark.read 
                   .format("csv")
                   .option("delimiter", ";")
                   .option("header", "true")
                   .option("inferSchema", "true")
                   .load("data/tieliikenneonnettomuudet_2015_onnettomuus.csv")
                   
 //df2.printSchema()
 val df3 : DataFrame  = spark.read 
                   .format("csv")
                   .option("delimiter", ";")
                   .option("header", "true")
                   .option("inferSchema", "true")
                   .load("data/tieliikenneonnettomuudet_2015_osallinen.csv")
                   
 //df3.printSchema()
                   
                   
 val vectorAssembler = new VectorAssembler()
                            .setInputCols(Array("X", "Y"))
                            .setOutputCol("features")    
 
 val transformationPipeline = new Pipeline()
                                  .setStages(Array(vectorAssembler))
 val coordinates : DataFrame = df2.select("X", "Y")                                  
 val pipeLine = transformationPipeline.fit(coordinates)
 val transformedTraining = pipeLine.transform(coordinates.na.drop)
 //transformedTraining.show
 val kmeans = new KMeans().setK(2).setSeed(1L)
 val kmModel = kmeans.fit(transformedTraining)
 //kmModel.summary.predictions.show
 
 /*kmModel.summary.predictions.select("X", "Y", "prediction")
 .coalesce(1)
 .write
 .format("com.databricks.spark.csv")
 .option("header", "true")
 .save("data/prova.csv")*/
 //.csv("data/prova.csv")
 
 
 //task2
  val weekIndexer = new StringIndexer().setInputCol("Vkpv").setOutputCol("dayOfWeek")
  
 val vectorAssembler1 = new VectorAssembler()
                            .setInputCols(Array("X", "Y", "dayOfWeek"))
                            .setOutputCol("features")    

 val transformationPipeline1 = new Pipeline()
                                  .setStages(Array(weekIndexer, vectorAssembler1 ))
                                  
 
 
 val coordinates1 : DataFrame = df2.select("X", "Y","Vkpv" )//.withColumnRenamed("Vkpv", "dayOfWeek")                                 
 val pipeLine1 = transformationPipeline1.fit(coordinates1)
 val transformedTraining1 = pipeLine1.transform(coordinates1.na.drop)
 //transformedTraining.show
 val kmeans1 = new KMeans().setK(2).setSeed(1L)
 val kmModel1 = kmeans1.fit(transformedTraining1)
 //kmModel1.summary.predictions.select("features").take(1).foreach(println)
 
 /*kmModel1.summary.predictions.select("X", "Y", "prediction")
 .coalesce(1)
 .write
 .format("com.databricks.spark.csv")
 .option("header", "true")
 .save("data/prova.csv")*/
 
 //task3
 
 
 //task4
 
 def sumDistances (i : Int) : Double = {
 //for(i <- 1 to 200){
   println(i) 
   val kmeans2 = new KMeans().setK(i).setSeed(1L)
   val kmModel2 = kmeans2.fit(transformedTraining)
   val pred = kmModel2.summary.predictions //.groupBy("prediction").agg(mean("X").alias("avgx"), mean("Y").alias("avgy")).show
   val avg : DataFrame= pred.groupBy("prediction").agg(mean("X").alias("avgx"), mean("Y").alias("avgy"))
   val complete : DataFrame= pred.join(avg, pred("prediction") === avg("prediction"), "left_outer")
   return complete
   .agg(sum((col("X") - col("avgx"))*(col("X") - col("avgx")) + (col("Y") - col("avgy"))*(col("Y") - col("avgy"))).cast("double")).first.getDouble(0)
   
   
   val weights : DataFrame= complete
   .withColumn("w", (col("X") - col("avgx"))*(col("X") - col("avgx")) + (col("Y") - col("avgy"))*(col("Y") - col("avgy")))
   
    return weights.agg(sum("w").alias("total").cast("double")).first.getDouble(0)
   
  
 }
  
  val range  = 2 to 200 toList
  val res = range.map(x => sumDistances(x)).zipWithIndex
  val res1 = spark.createDataFrame(res).toDF("dist", "ind")
  
  res1.coalesce(1)
 .write
 .format("com.databricks.spark.csv")
 .option("header", "true")
 .save("data/rrr.csv")
  
  
 /* val resSchema = Seq(
  StructField("dist", DoubleType, true),
  StructField("num", IntegerType, true)
  )
  
  //val res1 = spark.sparkContext.parallelize(res).toDF("dist", "ind")
  val res123 = spark.createDataFrame(
  spark.sparkContext.parallelize(res),
  StructType(resSchema)
  )*/
  
  //task 5
 /* def day (data_s : String) : Int = {
    data_s match {
      case MAANANTAI => 1
      case TIISTAI => 2
      case KESKIVIIKKO => 3
      case TORSTAI => 4
      case PERJANTAI => 5
      case LAUANTAI =>  6
      case SUNNUNTAI => 7
    }
  }
  
  val MAANANTAI = "Maanantai"
  val  TIISTAI = "Tiistai"
  val KESKIVIIKKO = "Keskiviikko"
  val TORSTAI = "Torstai"
  val PERJANTAI = "Perjantai"
  val LAUANTAI = "Lauantai"
  val SUNNUNTAI = "Sunnuntai"*/
  
  val someData = Seq(
  Row("Maanantai" : String, 1.0 : Double),
  Row("Tiistai" : String, 2.0 : Double),
  Row("Keskiviikko" : String,3.0 : Double),
  Row("Torstai" : String,4.0 : Double),
  Row("Perjantai" : String,5.0 : Double),
  Row("Lauantai" : String,6.0 : Double),
  Row("Sunnuntai" : String,7.0 : Double)
)

val someSchema = List(
  StructField("weekDay", StringType, true),
  StructField("num", DoubleType, true)
)

val someDF = spark.createDataFrame(
  spark.sparkContext.parallelize(someData),
  StructType(someSchema)
)



  val df5 : DataFrame= df2.join(someDF, df2("Vkpv") === someDF("weekDay"), "left_outer")
  //val df4 : DataFrame= df5.select("X", "Y", "num").na.drop
  val df4 : DataFrame= df5.select("X", "Y").na.drop
  val rows: RDD[Row] = df4.rdd

   
  val num : Int = 3
  
  val points = rows.take(3).zipWithIndex
  points.foreach(println)
  
  /*val d = rows.map(x => math.pow(x(0).asInstanceOf[Int] - points(1)(0).asInstanceOf[Int],2) +
                        math.pow(x(1).asInstanceOf[Int] - points(1)(1).asInstanceOf[Int],2)  
                        //math.pow(x.getInt(3) - points(1).getInt(3),2)
                          
  )
  d.foreach(println)  */// funziona
  
  val d = rows.map(x => ((points.map( y => ( y._2,
                        math.pow(x(0).asInstanceOf[Int] - y._1(0).asInstanceOf[Int],2) +
                        math.pow(x(1).asInstanceOf[Int] - y._1(1).asInstanceOf[Int],2) ) 
                        //math.pow(x.getInt(3) - points(1).getInt(3),2)
                          
  ).minBy(_._2)._1), (x(0).asInstanceOf[Int], x(1).asInstanceOf[Int], 1)))
  //d.foreach(println)
  
  val new_points = d.reduceByKey((x,y) => (x._1 + y._1, x._2 +y._2, x._3+y._3)).map(x =>(x._1, x._2._1/x._2._3, x._2._2/x._2._3))
  new_points.foreach(println)
  
  
  
  //se sono uguali allora mappo
  
  
  
  
  //task 6 // aggrego anche i punti oltre alla 
  
  //val elbow_dist =  newpoints.reduce((x,y) =>)
  
  
  
  //if (points == new_points)
  
  //val avg_points = 
  
  /*val dist = rows.map( x=> points.map( i => (x(0).asInstanceOf[Int] - i(0).asInstanceOf[Int])*(x(0).asInstanceOf[Int] - i(0).asInstanceOf[Int]) 
      + (x(1).asInstanceOf[Int] - i(1).asInstanceOf[Int])*(x(1).asInstanceOf[Int] - i(1).asInstanceOf[Int])
      + (x(2).asInstanceOf[Int] - i(2).asInstanceOf[Int])*(x(2).asInstanceOf[Int] - i(2).asInstanceOf[Int])
      ).min)
      
  dist.foreach(println)    */
  //dist.foreach(println)
  
  
  
    
    
    
 
  
  
 
  
   
 
 
  
  
}