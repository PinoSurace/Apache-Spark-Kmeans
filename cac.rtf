{\rtf1\ansi\ansicpg1252\cocoartf1671
{\fonttbl\f0\fnil\fcharset0 Monaco;}
{\colortbl;\red255\green255\blue255;\red127\green0\blue85;\red50\green147\blue153;\red162\green46\blue0;
\red76\green76\blue76;\red0\green0\blue192;\red63\green127\blue95;\red100\green0\blue103;\red94\green94\blue255;
\red196\green140\blue255;\red42\green0\blue255;}
{\*\expandedcolortbl;;\csgenericrgb\c49804\c0\c33333;\csgenericrgb\c19608\c57647\c60000;\csgenericrgb\c63529\c18039\c0;
\csgenericrgb\c29804\c29804\c29804;\csgenericrgb\c0\c0\c75294;\csgenericrgb\c24706\c49804\c37255;\csgenericrgb\c39216\c0\c40392;\csgenericrgb\c36863\c36863\c100000;
\csgenericrgb\c76863\c54902\c100000;\csgenericrgb\c16471\c0\c100000;}
\margl1440\margr1440\vieww14200\viewh16000\viewkind0
\pard\tx720\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\pardirnatural\partightenfactor0

\f0\fs22 \cf0 \
\pard\pardeftab720\partightenfactor0
\cf2 import\cf0  org.apache.spark.SparkConf\
\
\cf2 import\cf0  org.apache.spark.sql.SQLContext\
\cf2 import\cf0  org.apache.spark.sql.Column\
\cf2 import\cf0  org.apache.spark.sql.\cf3 Row\cf0 \
\cf2 import\cf0  org.apache.spark.sql.\cf3 DataFrame\cf0 \
\cf2 import\cf0  org.apache.spark.sql.types.\cf4 StructType\cf0 \
\cf2 import\cf0  org.apache.spark.sql.types.\{\cf4 ArrayType\cf0 , StringType, \cf4 StructField\cf0 \}\
\cf2 import\cf0  org.apache.spark.sql.types.DataType\
\cf2 import\cf0  org.apache.spark.sql.functions.\cf5 col\cf0 \
\cf2 import\cf0  org.apache.spark.sql.functions.\{\cf5 sum\cf0 , \cf5 min\cf0 , \cf5 max\cf0 , \cf5 asc\cf0 , \cf5 desc\cf0 , \cf5 udf\cf0 \}\
\
\cf2 import\cf0  org.apache.spark.sql.functions.\cf5 explode\cf0 \
\cf2 import\cf0  org.apache.spark.sql.functions.\cf5 array\cf0 \
\cf2 import\cf0  org.apache.spark.sql.SparkSession\
\
\cf2 import\cf0  com.databricks.spark.xml._\
\cf2 import\cf0  org.apache.spark.storage.StorageLevel\
\cf2 import\cf0  org.apache.spark.streaming.\{\cf3 Seconds\cf0 , StreamingContext\}\
\
\cf2 import\cf0  java.lang.Thread\
\
\cf2 import\cf0  org.apache.log4j.Logger\
\cf2 import\cf0  org.apache.log4j.Level\
\
\cf2 import\cf0  org.apache.spark._\
\cf2 import\cf0  org.apache.spark.SparkContext._\
\cf2 import\cf0  scala.util.Random\
\
\cf2 import\cf0  scala.annotation.tailrec\
\cf2 import\cf0  scala.collection._\
\cf2 import\cf0  org.apache.spark.rdd.RDD\
\
\
\cf2 case\cf0  \cf2 class\cf0  \cf4 Point\cf0 (\cf6 x\cf0 : Double, \cf6 y\cf0 : Double, \cf6 day\cf0 : Int)\
\
\cf2 class\cf0  Kmeans \{\
  \
  \cf7 // read data and process\cf0 \
  \cf2 def\cf0  \cf5 readData\cf0 (\cf8 lines\cf0  : RDD[\cf3 String\cf0 ]) : RDD[\cf4 Point\cf0 ] = \{\
    \cf8 lines\cf0 .\cf5 mapPartitionsWithIndex\cf0 ((\cf8 i\cf0 , \cf8 it\cf0 ) => if (\cf8 i\cf5 ==\cf0 0) \cf8 it\cf0 .\cf5 drop\cf0 (1) else \cf8 it\cf0 ).\cf5 map\cf0 (\cf8 line\cf0  => \{\
      val \cf9 splitted\cf0  = \cf8 line\cf0 .\cf5 split\cf0 (";")\
      if (\cf9 splitted\cf0 (19) \cf5 ==\cf0  "Maanantai") \cf4 Point\cf0 (\cf9 \ul \ulc9 splitted\cf0 \ulc0 (\cf10 \ulc10 55\cf0 \ulc0 )\ulnone .\cf5 toDouble\cf0 , \cf9 \ul \ulc9 splitted\cf0 \ulc0 (\cf10 \ulc10 56\cf0 \ulc0 )\ulnone .\cf5 toDouble\cf0 , \cf10 1\cf0 )\
      \cf2 else\cf0  \cf2 if\cf0  (\cf9 splitted\cf0 (\cf10 19\cf0 ) \cf5 ==\cf0  \cf11 "Tiistai"\cf0 ) \cf4 Point\cf0 (\cf9 \ul \ulc9 splitted\cf0 \ulc0 (\cf10 \ulc10 55\cf0 \ulc0 )\ulnone .\cf5 toDouble\cf0 , \cf9 \ul \ulc9 splitted\cf0 \ulc0 (\cf10 \ulc10 56\cf0 \ulc0 )\ulnone .\cf5 toDouble\cf0 , \cf10 2\cf0 )\
      \cf2 else\cf0  \cf2 if\cf0  (\cf9 splitted\cf0 (\cf10 19\cf0 ) \cf5 ==\cf0  \cf11 "Keskiviikko"\cf0 ) \cf4 Point\cf0 (\cf9 \ul \ulc9 splitted\cf0 \ulc0 (\cf10 \ulc10 55\cf0 \ulc0 )\ulnone .\cf5 toDouble\cf0 , \cf9 \ul \ulc9 splitted\cf0 \ulc0 (\cf10 \ulc10 56\cf0 \ulc0 )\ulnone .\cf5 toDouble\cf0 , \cf10 3\cf0 )\
      \cf2 else\cf0  \cf2 if\cf0  (\cf9 splitted\cf0 (\cf10 19\cf0 ) \cf5 ==\cf0  \cf11 "Torstai"\cf0 ) \cf4 Point\cf0 (\cf9 \ul \ulc9 splitted\cf0 \ulc0 (\cf10 \ulc10 55\cf0 \ulc0 )\ulnone .\cf5 toDouble\cf0 , \cf9 \ul \ulc9 splitted\cf0 \ulc0 (\cf10 \ulc10 56\cf0 \ulc0 )\ulnone .\cf5 toDouble\cf0 , \cf10 4\cf0 )\
      \cf2 else\cf0  \cf2 if\cf0  (\cf9 splitted\cf0 (\cf10 19\cf0 ) \cf5 ==\cf0  \cf11 "Perjantai"\cf0 ) \cf4 Point\cf0 (\cf9 \ul \ulc9 splitted\cf0 \ulc0 (\cf10 \ulc10 55\cf0 \ulc0 )\ulnone .\cf5 toDouble\cf0 , \cf9 \ul \ulc9 splitted\cf0 \ulc0 (\cf10 \ulc10 56\cf0 \ulc0 )\ulnone .\cf5 toDouble\cf0 , \cf10 5\cf0 )\
      \cf2 else\cf0  \cf2 if\cf0  (\cf9 splitted\cf0 (\cf10 19\cf0 ) \cf5 ==\cf0  \cf11 "Lauantai"\cf0 ) \cf4 Point\cf0 (\cf9 \ul \ulc9 splitted\cf0 \ulc0 (\cf10 \ulc10 55\cf0 \ulc0 )\ulnone .\cf5 toDouble\cf0 , \cf9 \ul \ulc9 splitted\cf0 \ulc0 (\cf10 \ulc10 56\cf0 \ulc0 )\ulnone .\cf5 toDouble\cf0 , \cf10 6\cf0 )\
      \cf2 else\cf0  \cf2 if\cf0  (\cf9 splitted\cf0 (\cf10 19\cf0 ) \cf5 ==\cf0  \cf11 "Sunnuntai"\cf0 ) \cf4 Point\cf0 (\cf9 \ul \ulc9 splitted\cf0 \ulc0 (\cf10 \ulc10 55\cf0 \ulc0 )\ulnone .\cf5 toDouble\cf0 , \cf9 \ul \ulc9 splitted\cf0 \ulc0 (\cf10 \ulc10 56\cf0 \ulc0 )\ulnone .\cf5 toDouble\cf0 , \cf10 7\cf0 )\
      \cf2 else\cf0  \cf4 Point\cf0 (\cf9 \ul \ulc9 splitted\cf0 \ulc0 (\cf10 \ulc10 55\cf0 \ulc0 )\ulnone .\cf5 toDouble\cf0 , \cf9 \ul \ulc9 splitted\cf0 \ulc0 (\cf10 \ulc10 56\cf0 \ulc0 )\ulnone .\cf5 toDouble\cf0 , \cf10 0\cf0 )\
    \})\
  \}\
\}\
\
\
\cf2 object\cf0  \cf3 main\cf0  \cf2 extends\cf0  Kmeans \{\
  \
  \cf2 def\cf0  \cf5 main\cf0 (\cf8 args\cf0 : Array[\cf3 String\cf0 ]) \{\
    \cf7 // Suppress the log messages:\cf0 \
    Logger.\cf5 getLogger\cf0 (\cf11 "org"\cf0 ).\cf5 setLevel\cf0 (Level.\cf6 OFF\cf0 )\
  \
    \cf2 val\cf0  \cf9 spark\cf0  = \cf3 SparkSession\cf0 .\cf5 builder\cf0 ()\
                          .\cf5 appName\cf0 (\cf11 "ex2"\cf0 )\
                          .\cf5 config\cf0 (\cf11 "spark.driver.host"\cf0 , \cf11 "localhost"\cf0 )\
                          .\cf5 master\cf0 (\cf11 "local"\cf0 )\
                          .\cf5 getOrCreate\cf0 ()\
                          \
    \cf2 val\cf0  \cf9 lines\cf0  = \cf9 spark\cf0 .\cf6 sparkContext\cf0 .\cf5 textFile\cf0 (\cf11 "data/2015.csv"\cf0 )\
    \
\cf7 //    lines.take(10).foreach(println)\cf0 \
    \
    \cf2 val\cf0  \cf9 data\cf0  = \cf5 readData\cf0 (\cf9 lines\cf0 )\
    \cf9 \ul \ulc9 data\cf0 \ulc0 .\cf5 \ulc5 take\cf0 \ulc0 (\cf10 \ulc10 10\cf0 \ulc0 )\ulnone .\cf5 foreach\cf0 (\cf5 println\cf0 )\
   \
  \}\
\}\
\
}