
import org.apache.spark.SparkConf

import org.apache.spark.sql.SQLContext
import org.apache.spark.sql.Column
import org.apache.spark.sql.Row
import org.apache.spark.sql.DataFrame
import org.apache.spark.sql.types.StructType
import org.apache.spark.sql.types.{ArrayType, StringType, StructField}
import org.apache.spark.sql.types.DataType
import org.apache.spark.sql.functions.col
import org.apache.spark.sql.functions.{sum, min, max, asc, desc, udf}

import org.apache.spark.sql.functions.explode
import org.apache.spark.sql.functions.array
import org.apache.spark.sql.SparkSession

import com.databricks.spark.xml._
import org.apache.spark.storage.StorageLevel
import org.apache.spark.streaming.{Seconds, StreamingContext}

import java.lang.Thread

import org.apache.log4j.Logger
import org.apache.log4j.Level

import org.apache.spark._
import org.apache.spark.SparkContext._
import scala.util.Random

import scala.annotation.tailrec
import scala.collection._
import org.apache.spark.rdd.RDD
import java.util.Date
import java.text.SimpleDateFormat
import Math._
import scala.util.Random
import scala.collection.immutable.HashSet
import scala.util.control.Breaks._
import java.io.PrintWriter


case class Point(x: Double, y: Double, day: Double)
case class RescaleInfo(minX : Double, maxX : Double, minY : Double, maxY : Double)

class Kmeans extends Serializable{
  /** number of clusters */
  def num_clusters = 10
  
  /** number of interations */
  def max_iterations = 50
  
  /** read data and process */
  def readData(lines: RDD[String]) : RDD[Point] = {
    val withoutHeader = lines.mapPartitionsWithIndex((i, it) => if (i==0) it.drop(1) else it)
    val dropBlankLine = withoutHeader.filter(x => !x.split(";")(55).isEmpty) // drop the line where x is None
    val processed_data = dropBlankLine.map(line => {
      val splitted = line.split(";")
      if (splitted(19) == "Maanantai") Point(splitted(55).toDouble, splitted(56).toDouble, 1)
      else if (splitted(19) == "Tiistai") Point(splitted(55).toDouble, splitted(56).toDouble, 2)
      else if (splitted(19) == "Keskiviikko") Point(splitted(55).toDouble, splitted(56).toDouble, 3)
      else if (splitted(19) == "Torstai") Point(splitted(55).toDouble, splitted(56).toDouble, 4)
      else if (splitted(19) == "Perjantai") Point(splitted(55).toDouble, splitted(56).toDouble, 5)
      else if (splitted(19) == "Lauantai") Point(splitted(55).toDouble, splitted(56).toDouble, 6)
      else if (splitted(19) == "Sunnuntai") Point(splitted(55).toDouble, splitted(56).toDouble, 7)
      else Point(splitted(55).toDouble, splitted(56).toDouble, 0)
    })
    return processed_data
  }
  
  /** Do data scaling because the 3 columns of the data are not in the same scale */
  def minMaxScaling(points: RDD[Point]) : (RDD[Point], RescaleInfo) = {
    val X_arr = points.map(_.x).collect()
    val minX = X_arr.reduceLeft(_ min _)
    val maxX = X_arr.reduceLeft(_ max _)
    val Y_arr = points.map(_.y).collect()
    val minY = Y_arr.reduceLeft(_ min _)
    val maxY = Y_arr.reduceLeft(_ max _)
    
    val rescale_info = RescaleInfo(minX, maxX, minY, maxY)
    
    val scaledPoints = points.map(aPoint => {
      val scaled_x = (aPoint.x - minX) / (maxX - minX)
      val scaled_y = (aPoint.y - minY) / (maxX - minX)
      val scaled_day = (aPoint.day - 1) / 6   // we only have days from 1 to 7
      Point(scaled_x, scaled_y, scaled_day)
    })
    return (scaledPoints, rescale_info)
  }
    
  /** initialize random centroids */
  def initCentroids(points: RDD[Point]) : Array[Point] = { 
    val pointsWithIndex = points.zipWithIndex().map{case (value, index) => (index, value)} // give indexes to the Points
    var randomCentroids = Array[Point]()
    
    val r = scala.util.Random
    for (i <- 0 until num_clusters) {
      val randomIdx = r.nextInt(points.count().toInt)
      val randomPoint = pointsWithIndex.lookup(randomIdx).head
      randomCentroids :+= randomPoint
    }
    
    return randomCentroids
  }
  
  /** Find Euclidean Distance between 2 points */
  def euclideanDistance(p1 : Point, p2 : Point) : Double = {
    val point1 = Array(p1.x, p1.y, p1.day)
    val point2 = Array(p2.x, p2.y, p2.day)
    // square root the: sum of: (zip point1 and point2 into pairs, for each pair, square the difference)
    val distance = sqrt((point1 zip point2).map { case (x,y) => pow(y - x, 2) }.sum)
    return distance
  }
  
  /** Find the index of the closest centroids for each point */
  def findClosestCentroid(point: Point, centroids: Array[Point]) : Int = {
    var bestIndex = 0
    var closest = Double.PositiveInfinity
    for (i <- 0 until centroids.length) {
      val tempDist = euclideanDistance(point, centroids(i))
      if (tempDist < closest) {
        closest = tempDist
        bestIndex = i
      }
    }
    return bestIndex
  }
  
  /** update the new centroids based on the points that are just clustered */
  def updateCentroid(groupOfPoints: Iterable[Point]) : Point = {
    val averaged_x = groupOfPoints.map(_.x).sum / groupOfPoints.size
    val averaged_y = groupOfPoints.map(_.y).sum / groupOfPoints.size
    val averaged_day = groupOfPoints.map(_.day).sum / groupOfPoints.size
    val new_centroid = Point(averaged_x, averaged_y, averaged_day)
    return new_centroid
  }
  
  /** Recursively run until converge (use @tailrec to optimize the tail recursive function) */
  @tailrec final def kmeansRun(points: RDD[Point], centroids: Array[Point], iterationNum: Int = 1) : Array[Point] = {
    val clustered_points = points.map(aPoint => {
      val idx = findClosestCentroid(aPoint, centroids)
      (idx, aPoint)
    }) // each point now has an index correspond to its centroid (the closest center)
    
    val new_centroids = clustered_points.groupByKey()
                                        .mapValues(i => updateCentroid(i))
                                        .map {case (key, value) => value}
                                        .collect
                                        
    // now find the total distance between the old and new centroids
    var total_distance = 0.0
    for (i <- 0 until centroids.length) {
      total_distance += euclideanDistance(centroids(i), new_centroids(i))
    }
    if (iterationNum > max_iterations) {
      return new_centroids
    } else {
      kmeansRun(points, new_centroids, iterationNum+1)
    }
  }

  /** after the algorithm we need to rescale the data back to original points */
  def rescaling (scaled_centroids : Array[Point], rescaleInfo : RescaleInfo) : Array[Point] = {
    val org_centroids = scaled_centroids.map(p => {
      val org_x = p.x * (rescaleInfo.maxX - rescaleInfo.minX) + rescaleInfo.minX
      val org_y = p.y * (rescaleInfo.maxY - rescaleInfo.minY) + rescaleInfo.minY
      val org_day = p.day * 6 + 1
      Point(org_x, org_y, org_day)
    })
    return org_centroids
  }
  
  /** export to CSV */
  def toCSV(centroids: Array[Point], filename: String) {
    val header = Array("X", "Y", "Day")
    val rows = centroids.map(p => {
      Array(p.x.toString(), p.y.toString(), p.day.toString())
    })
    val allRows = header +: rows
    val csv = allRows.map(_.mkString(",")).mkString("\n")
    new PrintWriter(filename) {write(csv); close()}
  }
  
  
}


object main extends Kmeans {
  
  def main(args: Array[String]) {
    // Suppress the log messages:
    Logger.getLogger("org").setLevel(Level.OFF)
  
    val spark = SparkSession.builder()
                          .appName("ex2")
                          .config("spark.driver.host", "localhost")
                          .master("local")
                          .getOrCreate()
                          
    val lines = spark.sparkContext.textFile("data/2015.csv")

    
    println("-------------Original Data---------------")
    val data = readData(lines)
    data.take(10).foreach(println)
    println()
    
    println("-------------Scaled data-----------------")
    val scaled_data = minMaxScaling(data)
    val scaled_points = scaled_data._1
    scaled_points.take(10).foreach(println)
    println()
    println("-----------Rescaled Info: minX, maxX, minY, maxY--------------")
    val rescale_info = scaled_data._2
    println(rescale_info)
    println()
    
    println("-------------Random Initialized Centroids-----------------")
    val randomCentroids = initCentroids(scaled_points)
    randomCentroids.foreach(println)
    println()
        
    println("-------------Run K-Means----------------")
    val scaled_centroids = kmeansRun(scaled_points, randomCentroids)
    scaled_centroids.foreach(println)
    println()
    
    println("-------------Rescale----------------")
    val rescaled_centroids = rescaling(scaled_centroids, rescale_info)
    rescaled_centroids.foreach(println)
    println()
    
    println("-------------Export to CSV----------------")
    toCSV(rescaled_centroids, "output/task5.csv")
    println("Done!!!!!!!!!!!!!!!")
    

  }
}

