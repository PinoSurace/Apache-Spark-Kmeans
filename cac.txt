
import org.apache.spark.SparkConf

import org.apache.spark.sql.SQLContext
import org.apache.spark.sql.Column
import org.apache.spark.sql.Row
import org.apache.spark.sql.DataFrame
import org.apache.spark.sql.types.StructType
import org.apache.spark.sql.types.{ArrayType, StringType, StructField}
import org.apache.spark.sql.types.DataType
import org.apache.spark.sql.functions.col
import org.apache.spark.sql.functions.{sum, min, max, asc, desc, udf}

import org.apache.spark.sql.functions.explode
import org.apache.spark.sql.functions.array
import org.apache.spark.sql.SparkSession

import com.databricks.spark.xml._
import org.apache.spark.storage.StorageLevel
import org.apache.spark.streaming.{Seconds, StreamingContext}

import java.lang.Thread

import org.apache.log4j.Logger
import org.apache.log4j.Level

import org.apache.spark._
import org.apache.spark.SparkContext._
import scala.util.Random

import scala.annotation.tailrec
import scala.collection._
import org.apache.spark.rdd.RDD


case class Point(x: Double, y: Double, day: Int)

class Kmeans {
  
  // read data and process
  def readData(lines : RDD[String]) : RDD[Point] = {
    lines.mapPartitionsWithIndex((i, it) => if (i==0) it.drop(1) else it).map(line => {
      val splitted = line.split(";")
      if (splitted(19) == "Maanantai") Point(splitted(55).toDouble, splitted(56).toDouble, 1)
      else if (splitted(19) == "Tiistai") Point(splitted(55).toDouble, splitted(56).toDouble, 2)
      else if (splitted(19) == "Keskiviikko") Point(splitted(55).toDouble, splitted(56).toDouble, 3)
      else if (splitted(19) == "Torstai") Point(splitted(55).toDouble, splitted(56).toDouble, 4)
      else if (splitted(19) == "Perjantai") Point(splitted(55).toDouble, splitted(56).toDouble, 5)
      else if (splitted(19) == "Lauantai") Point(splitted(55).toDouble, splitted(56).toDouble, 6)
      else if (splitted(19) == "Sunnuntai") Point(splitted(55).toDouble, splitted(56).toDouble, 7)
      else Point(splitted(55).toDouble, splitted(56).toDouble, 0)
    })
  }
}


object main extends Kmeans {
  
  def main(args: Array[String]) {
    // Suppress the log messages:
    Logger.getLogger("org").setLevel(Level.OFF)
  
    val spark = SparkSession.builder()
                          .appName("ex2")
                          .config("spark.driver.host", "localhost")
                          .master("local")
                          .getOrCreate()
                          
    val lines = spark.sparkContext.textFile("data/2015.csv")
    
//    lines.take(10).foreach(println)
    
    val data = readData(lines)
    data.take(10).foreach(println)
   
  }
}

