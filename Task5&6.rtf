{\rtf1\ansi\ansicpg1252\cocoartf1671
{\fonttbl\f0\fnil\fcharset0 Monaco;}
{\colortbl;\red255\green255\blue255;\red127\green0\blue85;\red50\green147\blue153;\red162\green46\blue0;
\red76\green76\blue76;\red0\green0\blue192;\red63\green95\blue191;\red196\green140\blue255;\red100\green0\blue103;
\red94\green94\blue255;\red42\green0\blue255;\red63\green127\blue95;\red255\green94\blue94;\red222\green0\blue172;
}
{\*\expandedcolortbl;;\csgenericrgb\c49804\c0\c33333;\csgenericrgb\c19608\c57647\c60000;\csgenericrgb\c63529\c18039\c0;
\csgenericrgb\c29804\c29804\c29804;\csgenericrgb\c0\c0\c75294;\csgenericrgb\c24706\c37255\c74902;\csgenericrgb\c76863\c54902\c100000;\csgenericrgb\c39216\c0\c40392;
\csgenericrgb\c36863\c36863\c100000;\csgenericrgb\c16471\c0\c100000;\csgenericrgb\c24706\c49804\c37255;\csgenericrgb\c100000\c36863\c36863;\csgenericrgb\c87059\c0\c67451;
}
\margl1440\margr1440\vieww10800\viewh8400\viewkind0
\pard\tx720\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\pardirnatural\partightenfactor0

\f0\fs22 \cf0 \
\pard\pardeftab720\partightenfactor0
\cf2 import\cf0  org.apache.spark.SparkConf\
\
\cf2 import\cf0  org.apache.spark.sql.SQLContext\
\cf2 import\cf0  org.apache.spark.sql.Column\
\cf2 import\cf0  org.apache.spark.sql.\cf3 Row\cf0 \
\cf2 import\cf0  org.apache.spark.sql.\cf3 DataFrame\cf0 \
\cf2 import\cf0  org.apache.spark.sql.types.\cf4 StructType\cf0 \
\cf2 import\cf0  org.apache.spark.sql.types.\{\cf4 ArrayType\cf0 , StringType, \cf4 StructField\cf0 \}\
\cf2 import\cf0  org.apache.spark.sql.types.DataType\
\cf2 import\cf0  org.apache.spark.sql.functions.\cf5 col\cf0 \
\cf2 import\cf0  org.apache.spark.sql.functions.\{\cf5 sum\cf0 , \cf5 min\cf0 , \cf5 max\cf0 , \cf5 asc\cf0 , \cf5 desc\cf0 , \cf5 udf\cf0 \}\
\
\cf2 import\cf0  org.apache.spark.sql.functions.\cf5 explode\cf0 \
\cf2 import\cf0  org.apache.spark.sql.functions.\cf5 array\cf0 \
\cf2 import\cf0  org.apache.spark.sql.SparkSession\
\
\cf2 import\cf0  com.databricks.spark.xml._\
\cf2 import\cf0  org.apache.spark.storage.StorageLevel\
\cf2 import\cf0  org.apache.spark.streaming.\{\cf3 Seconds\cf0 , StreamingContext\}\
\
\cf2 import\cf0  java.lang.Thread\
\
\cf2 import\cf0  org.apache.log4j.Logger\
\cf2 import\cf0  org.apache.log4j.Level\
\
\cf2 import\cf0  org.apache.spark._\
\cf2 import\cf0  org.apache.spark.SparkContext._\
\cf2 import\cf0  scala.util.Random\
\
\cf2 import\cf0  scala.annotation.tailrec\
\cf2 import\cf0  scala.collection._\
\cf2 import\cf0  org.apache.spark.rdd.RDD\
\cf2 import\cf0  java.util.Date\
\cf2 import\cf0  java.text.SimpleDateFormat\
\cf2 import\cf0  Math._\
\cf2 import\cf0  scala.util.Random\
\cf2 import\cf0  scala.collection.immutable.HashSet\
\cf2 import\cf0  scala.util.control.Breaks._\
\cf2 import\cf0  java.io.PrintWriter\
\
\
\cf2 case\cf0  \cf2 class\cf0  \cf4 Point\cf0 (\cf6 x\cf0 : Double, \cf6 y\cf0 : Double, \cf6 day\cf0 : Double)\
\cf2 case\cf0  \cf2 class\cf0  \cf4 RescaleInfo\cf0 (\cf6 minX\cf0  : Double, \cf6 maxX\cf0  : Double, \cf6 minY\cf0  : Double, \cf6 maxY\cf0  : Double)\
\
\cf2 class\cf0  Kmeans \cf2 extends\cf0  \cf3 Serializable\cf0 \{\
  \cf7 /** number of clusters */\cf0 \
  \cf2 def\cf0  \cf5 default_num_clusters\cf0  = \cf8 3\cf0 \
  \
  \cf7 /** number of \ul interations\ulnone  */\cf0 \
  \cf2 def\cf0  \cf5 max_iterations\cf0  = \cf8 500\cf0 \
  \
  \cf7 /** convergence condition */\cf0 \
  \cf2 def\cf0  \cf5 convergence_condition\cf0  : Double = \cf8 1.2D\cf0 \
  \
  \cf7 /** read data and process */\cf0 \
  \cf2 def\cf0  \cf5 readData\cf0 (\cf9 lines\cf0 : RDD[\cf3 String\cf0 ]) : RDD[\cf4 Point\cf0 ] = \{\
    \cf2 val\cf0  \cf10 withoutHeader\cf0  = \cf9 lines\cf0 .\cf5 mapPartitionsWithIndex\cf0 ((\cf9 i\cf0 , \cf9 it\cf0 ) => \cf2 if\cf0  (\cf9 i\cf5 ==\cf8 0\cf0 ) \cf9 it\cf0 .\cf5 drop\cf0 (\cf8 1\cf0 ) \cf2 else\cf0  \cf9 it\cf0 )\
    \cf2 val\cf0  \cf10 dropBlankLine\cf0  = \cf10 withoutHeader\cf0 .\cf5 filter\cf0 (\cf9 x\cf0  => !\cf9 x\cf0 .\cf5 split\cf0 (\cf11 ";"\cf0 )(\cf8 55\cf0 ).\cf5 isEmpty\cf0 ) \cf12 // drop the line where x is None\cf0 \
    \cf2 val\cf0  \cf10 processed_data\cf0  = \cf10 dropBlankLine\cf0 .\cf5 map\cf0 (\cf9 line\cf0  => \{\
      \cf2 val\cf0  \cf10 splitted\cf0  = \cf9 line\cf0 .\cf5 split\cf0 (\cf11 ";"\cf0 )\
      \cf2 if\cf0  (\cf10 splitted\cf0 (\cf8 19\cf0 ) \cf5 ==\cf0  \cf11 "Maanantai"\cf0 ) \cf4 Point\cf0 (\cf10 \ul \ulc10 splitted\cf0 \ulc0 (\cf8 \ulc8 55\cf0 \ulc0 )\ulnone .\cf5 toDouble\cf0 , \cf10 \ul \ulc10 splitted\cf0 \ulc0 (\cf8 \ulc8 56\cf0 \ulc0 )\ulnone .\cf5 toDouble\cf0 , \cf8 1\cf0 )\
      \cf2 else\cf0  \cf2 if\cf0  (\cf10 splitted\cf0 (\cf8 19\cf0 ) \cf5 ==\cf0  \cf11 "Tiistai"\cf0 ) \cf4 Point\cf0 (\cf10 \ul \ulc10 splitted\cf0 \ulc0 (\cf8 \ulc8 55\cf0 \ulc0 )\ulnone .\cf5 toDouble\cf0 , \cf10 \ul \ulc10 splitted\cf0 \ulc0 (\cf8 \ulc8 56\cf0 \ulc0 )\ulnone .\cf5 toDouble\cf0 , \cf8 2\cf0 )\
      \cf2 else\cf0  \cf2 if\cf0  (\cf10 splitted\cf0 (\cf8 19\cf0 ) \cf5 ==\cf0  \cf11 "Keskiviikko"\cf0 ) \cf4 Point\cf0 (\cf10 \ul \ulc10 splitted\cf0 \ulc0 (\cf8 \ulc8 55\cf0 \ulc0 )\ulnone .\cf5 toDouble\cf0 , \cf10 \ul \ulc10 splitted\cf0 \ulc0 (\cf8 \ulc8 56\cf0 \ulc0 )\ulnone .\cf5 toDouble\cf0 , \cf8 3\cf0 )\
      \cf2 else\cf0  \cf2 if\cf0  (\cf10 splitted\cf0 (\cf8 19\cf0 ) \cf5 ==\cf0  \cf11 "Torstai"\cf0 ) \cf4 Point\cf0 (\cf10 \ul \ulc10 splitted\cf0 \ulc0 (\cf8 \ulc8 55\cf0 \ulc0 )\ulnone .\cf5 toDouble\cf0 , \cf10 \ul \ulc10 splitted\cf0 \ulc0 (\cf8 \ulc8 56\cf0 \ulc0 )\ulnone .\cf5 toDouble\cf0 , \cf8 4\cf0 )\
      \cf2 else\cf0  \cf2 if\cf0  (\cf10 splitted\cf0 (\cf8 19\cf0 ) \cf5 ==\cf0  \cf11 "Perjantai"\cf0 ) \cf4 Point\cf0 (\cf10 \ul \ulc10 splitted\cf0 \ulc0 (\cf8 \ulc8 55\cf0 \ulc0 )\ulnone .\cf5 toDouble\cf0 , \cf10 \ul \ulc10 splitted\cf0 \ulc0 (\cf8 \ulc8 56\cf0 \ulc0 )\ulnone .\cf5 toDouble\cf0 , \cf8 5\cf0 )\
      \cf2 else\cf0  \cf2 if\cf0  (\cf10 splitted\cf0 (\cf8 19\cf0 ) \cf5 ==\cf0  \cf11 "Lauantai"\cf0 ) \cf4 Point\cf0 (\cf10 \ul \ulc10 splitted\cf0 \ulc0 (\cf8 \ulc8 55\cf0 \ulc0 )\ulnone .\cf5 toDouble\cf0 , \cf10 \ul \ulc10 splitted\cf0 \ulc0 (\cf8 \ulc8 56\cf0 \ulc0 )\ulnone .\cf5 toDouble\cf0 , \cf8 6\cf0 )\
      \cf2 else\cf0  \cf2 if\cf0  (\cf10 splitted\cf0 (\cf8 19\cf0 ) \cf5 ==\cf0  \cf11 "Sunnuntai"\cf0 ) \cf4 Point\cf0 (\cf10 \ul \ulc10 splitted\cf0 \ulc0 (\cf8 \ulc8 55\cf0 \ulc0 )\ulnone .\cf5 toDouble\cf0 , \cf10 \ul \ulc10 splitted\cf0 \ulc0 (\cf8 \ulc8 56\cf0 \ulc0 )\ulnone .\cf5 toDouble\cf0 , \cf8 7\cf0 )\
      \cf2 else\cf0  \cf4 Point\cf0 (\cf10 \ul \ulc10 splitted\cf0 \ulc0 (\cf8 \ulc8 55\cf0 \ulc0 )\ulnone .\cf5 toDouble\cf0 , \cf10 \ul \ulc10 splitted\cf0 \ulc0 (\cf8 \ulc8 56\cf0 \ulc0 )\ulnone .\cf5 toDouble\cf0 , \cf8 0\cf0 )\
    \})\
    \cf2 return\cf0  \cf10 processed_data\cf0 \
  \}\
  \
  \cf7 /** Do data scaling because the 3 columns of the data are not in the same scale */\cf0 \
  \cf2 def\cf0  \cf5 minMaxScaling\cf0 (\cf9 points\cf0 : RDD[\cf4 Point\cf0 ]) : (RDD[\cf4 Point\cf0 ], \cf4 RescaleInfo\cf0 ) = \{\
    \cf2 val\cf0  \cf10 X_arr\cf0  = \cf9 points\cf0 .\cf5 map\cf0 (_.\cf6 x\cf0 ).\cf5 collect\cf0 ()\
    \cf2 val\cf0  \cf10 minX\cf0  = \cf10 \ul \ulc10 X_arr\cf0 \ulnone .\cf5 reduceLeft\cf0 (\ul _\ulnone  \cf5 min\cf0  _)\
    \cf2 val\cf0  \cf10 maxX\cf0  = \cf10 \ul \ulc10 X_arr\cf0 \ulnone .\cf5 reduceLeft\cf0 (\ul _\ulnone  \cf5 max\cf0  _)\
    \cf2 val\cf0  \cf10 Y_arr\cf0  = \cf9 points\cf0 .\cf5 map\cf0 (_.\cf6 y\cf0 ).\cf5 collect\cf0 ()\
    \cf2 val\cf0  \cf10 minY\cf0  = \cf10 \ul \ulc10 Y_arr\cf0 \ulnone .\cf5 reduceLeft\cf0 (\ul _\ulnone  \cf5 min\cf0  _)\
    \cf2 val\cf0  \cf10 maxY\cf0  = \cf10 \ul \ulc10 Y_arr\cf0 \ulnone .\cf5 reduceLeft\cf0 (\ul _\ulnone  \cf5 max\cf0  _)\
    \
    \cf2 val\cf0  \cf10 rescale_info\cf0  = \cf4 RescaleInfo\cf0 (\cf10 minX\cf0 , \cf10 maxX\cf0 , \cf10 minY\cf0 , \cf10 maxY\cf0 )\
    \
    \cf2 val\cf0  \cf10 scaledPoints\cf0  = \cf9 points\cf0 .\cf5 map\cf0 (\cf9 aPoint\cf0  => \{\
      \cf2 val\cf0  \cf10 scaled_x\cf0  = (\cf9 aPoint\cf0 .\cf6 x\cf0  \cf5 -\cf0  \cf10 minX\cf0 ) \cf5 /\cf0  (\cf10 maxX\cf0  \cf5 -\cf0  \cf10 minX\cf0 )\
      \cf2 val\cf0  \cf10 scaled_y\cf0  = (\cf9 aPoint\cf0 .\cf6 y\cf0  \cf5 -\cf0  \cf10 minY\cf0 ) \cf5 /\cf0  (\cf10 maxX\cf0  \cf5 -\cf0  \cf10 minX\cf0 )\
      \cf2 val\cf0  \cf10 scaled_day\cf0  = (\cf9 aPoint\cf0 .\cf6 day\cf0  \cf5 -\cf0  \cf8 1\cf0 ) \cf5 /\cf0  \cf8 6\cf0    \cf12 // we only have days from 1 to 7\cf0 \
      \cf4 Point\cf0 (\cf10 scaled_x\cf0 , \cf10 scaled_y\cf0 , \cf10 scaled_day\cf0 )\
    \})\
    \cf2 return\cf0  (\cf10 scaledPoints\cf0 , \cf10 rescale_info\cf0 )\
  \}\
    \
  \cf7 /** initialize random \ul centroids\ulnone  */\cf0 \
  \cf2 def\cf0  \cf5 initCentroids\cf0 (\cf9 points\cf0 : RDD[\cf4 Point\cf0 ], \cf9 number_of_clusters\cf0  : Int = \cf5 default_num_clusters\cf0 ) : Array[\cf4 Point\cf0 ] = \{ \
    \cf2 val\cf0  \cf10 pointsWithIndex\cf0  = \cf9 points\cf0 .\cf5 zipWithIndex\cf0 ().\cf5 map\cf0 \{\cf2 case\cf0  (\cf10 value\cf0 , \cf10 index\cf0 ) => (\cf10 index\cf0 , \cf10 value\cf0 )\} \cf12 // give indexes to the Points\cf0 \
    \cf2 var\cf0  \cf13 randomCentroids\cf0  = \cf3 Array\cf0 [\cf4 Point\cf0 ]()\
    \
    \cf2 val\cf0  \cf10 r\cf0  = scala.util.\cf3 Random\cf0 \
    \cf2 for\cf0  (\cf10 i\cf0  <- \cf8 \ul \ulc8 0\cf0 \ulnone  \cf5 until\cf0  \cf9 number_of_clusters\cf0 ) \{\
      \cf2 val\cf0  \cf10 randomIdx\cf0  = \cf10 r\cf0 .\cf5 nextInt\cf0 (\cf9 points\cf0 .\cf5 count\cf0 ().\cf5 toInt\cf0 )\
      \cf2 val\cf0  \cf10 randomPoint\cf0  = \cf10 \ul \ulc10 pointsWithIndex\cf0 \ulnone .\cf5 lookup\cf0 (\cf10 randomIdx\cf0 ).\cf5 head\cf0 \
      \cf13 randomCentroids\cf0  :+= \cf10 randomPoint\cf0 \
    \}\
    \
    \cf2 return\cf0  \cf13 randomCentroids\cf0 \
  \}\
  \
  \cf7 /** Find \ul Euclidean\ulnone  Distance between 2 points */\cf0 \
  \cf2 def\cf0  \cf5 euclideanDistance\cf0 (\cf9 p1\cf0  : \cf4 Point\cf0 , \cf9 p2\cf0  : \cf4 Point\cf0 ) : Double = \{\
    \cf2 val\cf0  \cf10 point1\cf0  = \cf3 Array\cf0 (\cf9 p1\cf0 .\cf6 x\cf0 , \cf9 p1\cf0 .\cf6 y\cf0 , \cf9 p1\cf0 .\cf6 day\cf0 )\
    \cf2 val\cf0  \cf10 point2\cf0  = \cf3 Array\cf0 (\cf9 p2\cf0 .\cf6 x\cf0 , \cf9 p2\cf0 .\cf6 y\cf0 , \cf9 p2\cf0 .\cf6 day\cf0 )\
    \cf12 // square root the: sum of: (\ul zip\ulnone  point1 and point2 into pairs, for each pair, square the difference)\cf0 \
    \cf2 val\cf0  \cf10 distance\cf0  = \cf5 sqrt\cf0 ((\cf10 \ul \ulc10 point1\cf0 \ulc0  \cf5 \ulc5 zip\cf0 \ulc0  \cf10 \ulc10 point2\cf0 \ulc0 ).\cf5 \ulc5 map\cf0 \ulc0  \{ \cf2 \ulc2 case\cf0 \ulc0  (\cf10 \ulc10 x\cf0 \ulc0 ,\cf10 \ulc10 y\cf0 \ulc0 ) => \cf5 \ulc5 pow\cf0 \ulc0 (\cf10 \ulc10 y\cf0 \ulc0  \cf5 \ulc5 -\cf0 \ulc0  \cf10 \ulc10 x\cf0 \ulc0 , \cf8 \ulc8 2\cf0 \ulc0 ) \}\ulnone .\cf5 sum\cf0 )\
    \cf2 return\cf0  \cf10 distance\cf0 \
  \}\
  \
  \cf7 /** Find the index of the closest \ul centroids\ulnone  for each point */\cf0 \
  \cf2 def\cf0  \cf5 findClosestCentroid\cf0 (\cf9 point\cf0 : \cf4 Point\cf0 , \cf9 centroids\cf0 : Array[\cf4 Point\cf0 ]) : Int = \{\
    \cf2 var\cf0  \cf13 bestIndex\cf0  = \cf8 0\cf0 \
    \cf2 var\cf0  \cf13 closest\cf0  = \cf3 Double\cf0 .\cf6 PositiveInfinity\cf0 \
    \cf2 for\cf0  (\cf10 i\cf0  <- \cf8 \ul \ulc8 0\cf0 \ulnone  \cf5 until\cf0  \cf9 centroids\cf0 .\cf5 length\cf0 ) \{\
      \cf2 val\cf0  \cf10 tempDist\cf0  = \cf5 euclideanDistance\cf0 (\cf9 point\cf0 , \cf9 centroids\cf0 (\cf10 i\cf0 ))\
      \cf2 if\cf0  (\cf10 tempDist\cf0  \cf5 <\cf0  \cf13 closest\cf0 ) \{\
        \cf13 closest\cf0  = \cf10 tempDist\cf0 \
        \cf13 bestIndex\cf0  = \cf10 i\cf0 \
      \}\
    \}\
    \cf2 return\cf0  \cf13 bestIndex\cf0 \
  \}\
  \
  \cf7 /** update the new \ul centroids\ulnone  based on the points that are just clustered,\cf0 \
\cf7    *  also compute the sum of square error for each cluster */\cf0 \
  \cf2 def\cf0  \cf5 updateCentroid\cf0 (\cf9 aCluster\cf0 : \cf3 Iterable\cf0 [\cf4 Point\cf0 ]) : (\cf4 Point\cf0 , Double) = \{\
    \cf2 val\cf0  \cf10 averaged_x\cf0  = \cf9 aCluster\cf0 .\cf5 map\cf0 (_.\cf6 x\cf0 ).\cf5 sum\cf0  \cf5 /\cf0  \cf9 aCluster\cf0 .\cf5 size\cf0 \
    \cf2 val\cf0  \cf10 averaged_y\cf0  = \cf9 aCluster\cf0 .\cf5 map\cf0 (_.\cf6 y\cf0 ).\cf5 sum\cf0  \cf5 /\cf0  \cf9 aCluster\cf0 .\cf5 size\cf0 \
    \cf2 val\cf0  \cf10 averaged_day\cf0  = \cf9 aCluster\cf0 .\cf5 map\cf0 (_.\cf6 day\cf0 ).\cf5 sum\cf0  \cf5 /\cf0  \cf9 aCluster\cf0 .\cf5 size\cf0 \
    \cf2 val\cf0  \cf10 new_centroid\cf0  = \cf4 Point\cf0 (\cf10 averaged_x\cf0 , \cf10 averaged_y\cf0 , \cf10 averaged_day\cf0 )\
    \cf12 // find sum of square error for each cluster\cf0 \
    \cf2 val\cf0  \cf10 cost_x\cf0  = \cf9 aCluster\cf0 .\cf5 map\cf0 (\cf9 aPoint\cf0  => \cf5 pow\cf0 (\cf9 aPoint\cf0 .\cf6 x\cf0  \cf5 -\cf0  \cf10 averaged_x\cf0 , \cf8 2\cf0 )).\cf5 sum\cf0 \
    \cf2 val\cf0  \cf10 cost_y\cf0  = \cf9 aCluster\cf0 .\cf5 map\cf0 (\cf9 aPoint\cf0  => \cf5 pow\cf0 (\cf9 aPoint\cf0 .\cf6 y\cf0  \cf5 -\cf0  \cf10 averaged_y\cf0 , \cf8 2\cf0 )).\cf5 sum\cf0 \
    \cf2 val\cf0  \cf10 cost_day\cf0  = \cf9 aCluster\cf0 .\cf5 map\cf0 (\cf9 aPoint\cf0  => \cf5 pow\cf0 (\cf9 aPoint\cf0 .\cf6 x\cf0  \cf5 -\cf0  \cf10 averaged_day\cf0 , \cf8 2\cf0 )).\cf5 sum\cf0 \
    \cf2 val\cf0  \cf10 cost_cluster\cf0  = \cf10 cost_x\cf0  \cf5 +\cf0  \cf10 cost_y\cf0  \cf5 +\cf0  \cf10 cost_day\cf0 \
    \
    \cf2 return\cf0  (\cf10 new_centroid\cf0 , \cf10 cost_cluster\cf0 )\
  \}\
  \
  \cf7 /** Recursively run until converge (use \ul @tailrec\ulnone  to optimize the tail recursive function) */\cf0 \
  @\cf14 tailrec\cf0  \cf2 final\cf0  \cf2 def\cf0  \cf5 kmeansRun\cf0 (\cf9 points\cf0 : RDD[\cf4 Point\cf0 ], \cf9 centroids\cf0 : Array[\cf4 Point\cf0 ], \cf9 iterationNum\cf0 : Int = \cf8 1\cf0 ) : \
                                                                    (Array[\cf4 Point\cf0 ], RDD[(Int, \cf4 Point\cf0 )], Double) = \{\
    \cf12 // assign each point a \ul centroid\ulnone  index\cf0 \
    \cf2 val\cf0  \cf10 clustered_points\cf0  = \cf9 points\cf0 .\cf5 map\cf0 (\cf9 aPoint\cf0  => \{\
      \cf2 val\cf0  \cf10 idx\cf0  = \cf5 findClosestCentroid\cf0 (\cf9 aPoint\cf0 , \cf9 centroids\cf0 )\
      (\cf10 idx\cf0 , \cf9 aPoint\cf0 )\
    \}) \cf12 // each point now has an index correspond to its \ul centroid\ulnone  (the closest center)\cf0 \
    \
    \cf2 val\cf0  \cf10 indexed_centroids_with_error\cf0  = \cf10 \ul \ulc10 clustered_points\cf0 \ulc0 .\cf5 \ulc5 groupByKey\cf0 \ulc0 ()\ulnone \
                                                       .\cf5 mapValues\cf0 (\cf9 aCluster\cf0  => \cf5 updateCentroid\cf0 (\cf9 aCluster\cf0 ))\
                                               \
    \cf2 val\cf0  \cf10 new_centroids\cf0  = \cf10 indexed_centroids_with_error\cf0 .\cf5 map\cf0 \{ \cf2 case\cf0  (\cf10 key\cf0 , \cf10 value\cf0 ) => \cf10 value\cf0 .\cf6 _1\cf0  \}\
                                                    .\cf5 collect\cf0 ()\
    \
    \cf12 // find the sum of square error of all clusters (the cost function that we want to minimize with Elbow Algorithm)\cf0 \
    \cf2 val\cf0  \cf10 distortion\cf0  = \cf10 \ul \ulc10 indexed_centroids_with_error\cf0 \ulc0 .\cf5 \ulc5 map\cf0 \ulc0 \{ \cf2 \ulc2 case\cf0 \ulc0  (\cf10 \ulc10 key\cf0 \ulc0 , \cf10 \ulc10 value\cf0 \ulc0 ) => \cf10 \ulc10 value\cf0 \ulc0 .\cf6 \ulc6 _2\cf0 \ulc0  \}\ulnone .\cf5 sum\cf0                                      \
    \
                                        \
    \cf12 // now find the total distance between the old and new \ul centroids\cf0 \ulnone \
    \cf2 var\cf0  \cf13 total_distance\cf0  = \cf8 0.0\cf0 \
    \cf2 for\cf0  (\cf10 i\cf0  <- \cf8 \ul \ulc8 0\cf0 \ulnone  \cf5 until\cf0  \cf9 centroids\cf0 .\cf5 length\cf0 ) \{\
      \cf13 total_distance\cf0  += \cf5 euclideanDistance\cf0 (\cf9 centroids\cf0 (\cf10 i\cf0 ), \cf10 new_centroids\cf0 (\cf10 i\cf0 ))\
    \}\
    \cf12 // stop if there is no significant changes at the \ul centroid\ulnone  points or we exceed the number of iterations\cf0 \
    \cf2 if\cf0  (\cf13 total_distance\cf0  \cf5 <\cf0  \cf5 convergence_condition\cf0  \cf5 ||\cf0  \cf9 iterationNum\cf0  \cf5 >\cf0  \cf5 max_iterations\cf0 ) \{\
      \cf2 return\cf0  (\cf10 new_centroids\cf0 , \cf10 clustered_points\cf0 , \cf10 distortion\cf0 )\
    \} \cf2 else\cf0  \{\
      \cf5 kmeansRun\cf0 (\cf9 points\cf0 , \cf10 new_centroids\cf0 , \cf9 iterationNum\cf5 +\cf8 1\cf0 )\
    \}\
  \}\
\
  \cf7 /** \ul rescale\ulnone  the \ul centroids\ulnone  back to original coordinate */\cf0 \
  \cf2 def\cf0  \cf5 rescaling_centroids\cf0  (\cf9 scaled_centroids\cf0  : Array[\cf4 Point\cf0 ], \cf9 rescaleInfo\cf0  : \cf4 RescaleInfo\cf0 ) : Array[\cf4 Point\cf0 ] = \{\
    \cf2 val\cf0  \cf10 org_centroids\cf0  = \cf9 \ul \ulc9 scaled_centroids\cf0 \ulnone .\cf5 map\cf0 (\cf9 p\cf0  => \{\
      \cf2 val\cf0  \cf10 org_x\cf0  = \cf9 p\cf0 .\cf6 x\cf0  \cf5 *\cf0  (\cf9 rescaleInfo\cf0 .\cf6 maxX\cf0  \cf5 -\cf0  \cf9 rescaleInfo\cf0 .\cf6 minX\cf0 ) \cf5 +\cf0  \cf9 rescaleInfo\cf0 .\cf6 minX\cf0 \
      \cf2 val\cf0  \cf10 org_y\cf0  = \cf9 p\cf0 .\cf6 y\cf0  \cf5 *\cf0  (\cf9 rescaleInfo\cf0 .\cf6 maxY\cf0  \cf5 -\cf0  \cf9 rescaleInfo\cf0 .\cf6 minY\cf0 ) \cf5 +\cf0  \cf9 rescaleInfo\cf0 .\cf6 minY\cf0 \
      \cf2 val\cf0  \cf10 org_day\cf0  = \cf9 p\cf0 .\cf6 day\cf0  \cf5 *\cf0  \cf8 6\cf0  \cf5 +\cf0  \cf8 1\cf0 \
      \cf4 Point\cf0 (\cf10 org_x\cf0 , \cf10 org_y\cf0 , \cf10 org_day\cf0 )\
    \})\
    \cf2 return\cf0  \cf10 org_centroids\cf0 \
  \}\
  \
  \cf7 /** \ul rescale\ulnone  all the points back to original coordinate (to plot later) */\cf0 \
  \cf2 def\cf0  \cf5 rescaling_points\cf0 (\cf9 clustered_points\cf0 : RDD[(Int, \cf4 Point\cf0 )], \cf9 rescaleInfo\cf0 : \cf4 RescaleInfo\cf0 ) : RDD[(Int, \cf4 Point\cf0 )] = \{\
\cf12 //    \ul var\ulnone  res = Array[(\ul Int\ulnone , Point)]()\cf0 \
    \cf2 val\cf0  \cf10 org_points\cf0  = \cf9 clustered_points\cf0 .\cf5 map\cf0 (\cf9 point\cf0  => \{\
      \cf2 val\cf0  \cf10 org_x\cf0  = \cf9 point\cf0 .\cf6 _2\cf0 .\cf6 x\cf0  \cf5 *\cf0  (\cf9 rescaleInfo\cf0 .\cf6 maxX\cf0  \cf5 -\cf0  \cf9 rescaleInfo\cf0 .\cf6 minX\cf0 ) \cf5 +\cf0  \cf9 rescaleInfo\cf0 .\cf6 minX\cf0 \
      \cf2 val\cf0  \cf10 org_y\cf0  = \cf9 point\cf0 .\cf6 _2\cf0 .\cf6 y\cf0  \cf5 *\cf0  (\cf9 rescaleInfo\cf0 .\cf6 maxY\cf0  \cf5 -\cf0  \cf9 rescaleInfo\cf0 .\cf6 minY\cf0 ) \cf5 +\cf0  \cf9 rescaleInfo\cf0 .\cf6 minY\cf0 \
      \cf2 val\cf0  \cf10 org_day\cf0  = \cf9 point\cf0 .\cf6 _2\cf0 .\cf6 day\cf0  \cf5 *\cf0  \cf8 6\cf0  \cf5 +\cf0  \cf8 1\cf0 \
      (\cf9 point\cf0 .\cf6 _1\cf0 , \cf4 Point\cf0 (\cf10 org_x\cf0 , \cf10 org_y\cf0 , \cf10 org_day\cf0 ))\
    \})\
    \cf2 return\cf0  \cf10 org_points\cf0 \
  \}\
  \
  \cf7 /** export \ul centroids\ulnone  to CSV */\cf0 \
  \cf2 def\cf0  \cf5 toCSV_centroids\cf0 (\cf9 centroids\cf0 : Array[\cf4 Point\cf0 ], \cf9 filename\cf0 : \cf3 String\cf0  = \cf11 "output/task5.csv"\cf0 ) \{\
    \cf2 val\cf0  \cf10 header\cf0  = \cf3 Array\cf0 (\cf11 "X"\cf0 , \cf11 "Y"\cf0 , \cf11 "Day"\cf0 )\
    \cf2 val\cf0  \cf10 rows\cf0  = \cf9 \ul \ulc9 centroids\cf0 \ulnone .\cf5 map\cf0 (\cf9 p\cf0  => \{\
      \cf3 Array\cf0 (\cf9 p\cf0 .\cf6 x\cf0 .\cf5 toString\cf0 (), \cf9 p\cf0 .\cf6 y\cf0 .\cf5 toString\cf0 (), \cf9 p\cf0 .\cf6 day\cf0 .\cf5 toString\cf0 ())\
    \})\
    \cf2 val\cf0  \cf10 allRows\cf0  = \cf10 header\cf0  \cf5 +:\cf0  \cf10 \ul \ulc10 rows\cf0 \ulnone \
    \cf2 val\cf0  \cf10 csv\cf0  = \cf10 \ul \ulc10 allRows\cf0 \ulc0 .\cf5 \ulc5 map\cf0 \ulc0 (_.\cf5 \ulc5 mkString\cf0 \ulc0 (\cf11 \ulc11 ","\cf0 \ulc0 ))\ulnone .\cf5 mkString\cf0 (\cf11 "\cf8 \\n\cf11 "\cf0 )\
    \cf2 new\cf0  PrintWriter(\cf9 filename\cf0 ) \{\cf5 write\cf0 (\cf10 csv\cf0 ); \cf5 close\cf0 ()\}\
  \}\
  \
  \cf7 /** export clustered points to CSV */\cf0 \
  \cf2 def\cf0  \cf5 toCSV_points\cf0 (\cf9 points\cf0 : RDD[(Int, \cf4 Point\cf0 )], \cf9 filename\cf0 : \cf3 String\cf0  = \cf11 "output/task5_clusteredPoints.csv"\cf0 ) \{\
    \cf2 val\cf0  \cf10 header\cf0  = \cf3 Array\cf0 (\cf11 "Cluster"\cf0 , \cf11 "X"\cf0 , \cf11 "Y"\cf0 , \cf11 "Day"\cf0 )\
    \cf2 val\cf0  \cf10 rows\cf0  = \cf9 points\cf0 .\cf5 map\cf0 (\cf9 point\cf0  => \{\
      \cf3 Array\cf0 (\cf9 point\cf0 .\cf6 _1\cf0 .\cf5 toString\cf0 (), \cf9 point\cf0 .\cf6 _2\cf0 .\cf6 x\cf0 .\cf5 toString\cf0 (), \cf9 point\cf0 .\cf6 _2\cf0 .\cf6 y\cf0 .\cf5 toString\cf0 (), \cf9 point\cf0 .\cf6 _2\cf0 .\cf6 day\cf0 .\cf5 toString\cf0 ())\
    \}).\cf5 collect\cf0 ()\
    \cf2 val\cf0  \cf10 allRows\cf0  = \cf10 header\cf0  \cf5 +:\cf0  \cf10 \ul \ulc10 rows\cf0 \ulnone \
    \cf2 val\cf0  \cf10 csv\cf0  = \cf10 \ul \ulc10 allRows\cf0 \ulc0 .\cf5 \ulc5 map\cf0 \ulc0 (_.\cf5 \ulc5 mkString\cf0 \ulc0 (\cf11 \ulc11 ","\cf0 \ulc0 ))\ulnone .\cf5 mkString\cf0 (\cf11 "\cf8 \\n\cf11 "\cf0 )\
    \cf2 new\cf0  PrintWriter(\cf9 filename\cf0 ) \{\cf5 write\cf0 (\cf10 csv\cf0 ); \cf5 close\cf0 ()\}\
  \}\
  \
  \cf2 def\cf0  \cf5 toCSV_elbow\cf0 (\cf9 costs\cf0  : Array[(Int, Double)], \cf9 filename\cf0 : \cf3 String\cf0  = \cf11 "output/task6_CostWithDifferentNumberOfClusters.csv"\cf0 ) \{\
    \cf2 val\cf0  \cf10 header\cf0  = \cf3 Array\cf0 (\cf11 "NumOfCluster"\cf0 , \cf11 "Cost"\cf0 )\
    \cf2 val\cf0  \cf10 rows\cf0  = \cf9 \ul \ulc9 costs\cf0 \ulnone .\cf5 map\cf0 (\cf9 x\cf0  => \cf3 Array\cf0 (\cf9 x\cf0 .\cf6 _1\cf0 .\cf5 toString\cf0 (), \cf9 x\cf0 .\cf6 _2\cf0 .\cf5 toString\cf0 ()))\
    \cf2 val\cf0  \cf10 allRows\cf0  = \cf10 header\cf0  \cf5 +:\cf0  \cf10 \ul \ulc10 rows\cf0 \ulnone \
    \cf2 val\cf0  \cf10 csv\cf0  = \cf10 \ul \ulc10 allRows\cf0 \ulc0 .\cf5 \ulc5 map\cf0 \ulc0 (_.\cf5 \ulc5 mkString\cf0 \ulc0 (\cf11 \ulc11 ","\cf0 \ulc0 ))\ulnone .\cf5 mkString\cf0 (\cf11 "\cf8 \\n\cf11 "\cf0 )\
    \cf2 new\cf0  PrintWriter(\cf9 filename\cf0 ) \{\cf5 write\cf0 (\cf10 csv\cf0 ); \cf5 close\cf0 ()\}\
  \}\
\}\
\
\
\cf2 object\cf0  \cf3 main\cf0  \cf2 extends\cf0  Kmeans \{\
  \
  \cf2 def\cf0  \cf5 main\cf0 (\cf9 args\cf0 : Array[\cf3 String\cf0 ]) \{\
    \cf12 // Suppress the log messages:\cf0 \
    Logger.\cf5 getLogger\cf0 (\cf11 "org"\cf0 ).\cf5 setLevel\cf0 (Level.\cf6 OFF\cf0 )\
  \
    \cf2 val\cf0  \cf10 spark\cf0  = \cf3 SparkSession\cf0 .\cf5 builder\cf0 ()\
                          .\cf5 appName\cf0 (\cf11 "ex2"\cf0 )\
                          .\cf5 config\cf0 (\cf11 "spark.driver.host"\cf0 , \cf11 "localhost"\cf0 )\
                          .\cf5 master\cf0 (\cf11 "local"\cf0 )\
                          .\cf5 getOrCreate\cf0 ()\
                          \
    \cf2 val\cf0  \cf10 lines\cf0  = \cf10 spark\cf0 .\cf6 sparkContext\cf0 .\cf5 textFile\cf0 (\cf11 "data/2015.csv"\cf0 )\
\
    \cf12 // Task05\cf0 \
    \
    \cf5 println\cf0 (\cf11 "-------------Original Data---------------"\cf0 )\
    \cf2 val\cf0  \cf10 data\cf0  = \cf5 readData\cf0 (\cf10 lines\cf0 )\
    \cf10 \ul \ulc10 data\cf0 \ulc0 .\cf5 \ulc5 take\cf0 \ulc0 (\cf8 \ulc8 10\cf0 \ulc0 )\ulnone .\cf5 foreach\cf0 (\cf5 println\cf0 )\
    \cf5 println\cf0 ()\
    \
    \cf5 println\cf0 (\cf11 "-------------Scaled data-----------------"\cf0 )\
    \cf2 val\cf0  \cf10 scaled_data\cf0  = \cf5 minMaxScaling\cf0 (\cf10 data\cf0 )\
    \cf2 val\cf0  \cf10 scaled_points\cf0  = \cf10 scaled_data\cf0 .\cf6 _1\cf0 \
    \cf10 \ul \ulc10 scaled_points\cf0 \ulc0 .\cf5 \ulc5 take\cf0 \ulc0 (\cf8 \ulc8 10\cf0 \ulc0 )\ulnone .\cf5 foreach\cf0 (\cf5 println\cf0 )\
    \cf5 println\cf0 ()\
    \cf5 println\cf0 (\cf11 "-----------Rescaled Info: minX, maxX, minY, maxY--------------"\cf0 )\
    \cf2 val\cf0  \cf10 rescale_info\cf0  = \cf10 scaled_data\cf0 .\cf6 _2\cf0 \
    \cf5 println\cf0 (\cf10 rescale_info\cf0 )\
    \cf5 println\cf0 ()\
    \
    \cf5 println\cf0 (\cf11 "-------------Random Initialized Centroids-----------------"\cf0 )\
    \cf2 val\cf0  \cf10 randomCentroids\cf0  = \cf5 initCentroids\cf0 (\cf10 scaled_points\cf0 )\
    \cf10 \ul \ulc10 randomCentroids\cf0 \ulnone .\cf5 foreach\cf0 (\cf5 println\cf0 )\
    \cf5 println\cf0 ()\
        \
    \cf5 println\cf0 (\cf11 "-------------Run K-Means----------------"\cf0 )\
    \cf2 val\cf0  (\cf10 scaled_centroids\cf0 , \cf10 clustered_points\cf0 , \cf10 distortion\cf0 ) = \cf5 kmeansRun\cf0 (\cf10 scaled_points\cf0 , \cf10 randomCentroids\cf0 )\
    \cf10 \ul \ulc10 scaled_centroids\cf0 \ulnone .\cf5 foreach\cf0 (\cf5 println\cf0 )\
    \cf5 println\cf0 ()\
    \
    \cf5 println\cf0 (\cf11 "-------------Rescale----------------"\cf0 )\
    \cf5 println\cf0 (\cf11 "-------------Centroids---------------"\cf0 )\
    \cf2 val\cf0  \cf10 rescaled_centroids\cf0  = \cf5 rescaling_centroids\cf0 (\cf10 scaled_centroids\cf0 , \cf10 rescale_info\cf0 )\
    \cf10 \ul \ulc10 rescaled_centroids\cf0 \ulnone .\cf5 foreach\cf0 (\cf5 println\cf0 )\
    \cf5 println\cf0 (\cf11 "-------------Points---------------"\cf0 )\
    \cf2 val\cf0  \cf10 rescaled_clustered_points\cf0  = \cf5 rescaling_points\cf0 (\cf10 clustered_points\cf0 , \cf10 rescale_info\cf0 )\
    \cf10 \ul \ulc10 rescaled_clustered_points\cf0 \ulc0 .\cf5 \ulc5 take\cf0 \ulc0 (\cf8 \ulc8 20\cf0 \ulc0 )\ulnone .\cf5 foreach\cf0 (\cf5 println\cf0 )    \
    \cf5 println\cf0 ()\
    \
    \cf5 println\cf0 (\cf11 "-------------Export to CSV----------------"\cf0 )\
    \cf5 toCSV_centroids\cf0 (\cf10 rescaled_centroids\cf0 , \cf11 "output/task5.csv"\cf0 )\
    \cf5 toCSV_points\cf0 (\cf10 rescaled_clustered_points\cf0 , \cf11 "output/task5_clusteredPoints.csv"\cf0 )\
    \cf5 println\cf0 (\cf11 "!!!!!!!!!!!!!!!!!!!!Done!!!!!!!!!!!!!!!!!!!"\cf0 )\
    \
		\
    \
    \
    \cf12 // Task06\cf0 \
 \
\cf12 //    \ul println\ulnone ("-------------Original Data---------------")\cf0 \
\cf12 //    \ul val\ulnone  data = readData(lines)\cf0 \
\cf12 //    data.take(10).\ul foreach\ulnone (\ul println\ulnone )\cf0 \
\cf12 //    \ul println\ulnone ()\cf0 \
\cf12 //    \cf0 \
\cf12 //    \ul println\ulnone ("-------------Scaled data-----------------")\cf0 \
\cf12 //    \ul val\ulnone  scaled_data = minMaxScaling(data)\cf0 \
\cf12 //    \ul val\ulnone  scaled_points = scaled_data._1\cf0 \
\cf12 //    scaled_points.take(10).\ul foreach\ulnone (\ul println\ulnone )\cf0 \
\cf12 //    \ul println\ulnone ()\cf0 \
\cf12 //    \cf0 \
\cf12 //    \ul println\ulnone ("-------------Run \ul Kmeans\ulnone  with Different Number of Clusters-----------------")\cf0 \
\cf12 //    \ul var\ulnone  cost_function_values = Array[(\ul Int\ulnone ,Double)]()\cf0 \
\cf12 //    for (i <- 2 until 20) \{\cf0 \
\cf12 //      \ul val\ulnone  randomCentroids = initCentroids(scaled_points, i)\cf0 \
\cf12 //      \ul val\ulnone  (scaled_centroids, clustered_points, distortion) = kmeansRun(scaled_points, randomCentroids) \cf0 \
\cf12 //      cost_function_values :+= (i, distortion)\cf0 \
\cf12 //    \}\cf0 \
\cf12 //    cost_function_values.foreach(\ul println\ulnone )\cf0 \
\cf12 //    toCSV_elbow(cost_function_values)\cf0 \
    \
    \cf12 // Optimal number of Clusters: 3\cf0 \
    \
  \}\
\}\
\
}